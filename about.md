---
layout: page
title: About
description: >-
    Course policies and information.
---

# About
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Goals of the Course

Natural language processing (NLP) seeks to endow computers with the ability to intelligently process human language. NLP components are used in conversational agents and other systems that engage in dialogue with humans, automatic translation between human languages, automatic answering of questions using large text collections, the extraction of structured information from text, tools that help human authors, and many, many more. This course will teach you the <b>fundamental ideas</b> used in key NLP components. It is organized into several “greatest hits” topics, each with a prerecorded video lecture and associated readings, problems, and implementation exercises.


## 447 vs. 517

The courses are similar in breadth and use the same lecture content.  The projects are quite different; 447’s project is a predefined implementation problem that gives teams freedom in developing a solution.  It is designed to encourage iterative improvement and an understanding of inherent tradeoffs in building an NLP system.  517’s project is more research-oriented; it asks teams to reproduce experiments in recently published NLP papers.  Teams have great flexibility in the choice of a paper to reproduce.

Additionally, there will be differences in the assignments.


## Lectures

You are encouraged to come to live lectures and participate in the discussions.  Videorecorded lectures from 2021 are provided as a supplementary resource.

1. [Classification and multinomial logistic regression](https://drive.google.com/file/d/1Luwa-sn4t2Hu6IA_-cUWXaDvMkpft9E4/view?usp=sharing); [with captions](https://drive.google.com/file/d/1iRFKwz8IInkjDFWB5rU7RO9tGtVna6wF/view?usp=sharing); [transcript](https://drive.google.com/file/d/1cxtCdPySB1PL72EQSWJOy2tpGkf0kYWK/view?usp=sharing); [slides](https://drive.google.com/file/d/1u3hyvV7bnh11yY6jCOnKOzWyWU8yPw6u/view?usp=sharing)
1. [Language modeling, especially with neural networks](https://drive.google.com/file/d/1cK43rSzH491oI9NIrLlDAeP8P2F7LXTJ/view?usp=sharing); [with captions](https://drive.google.com/file/d/17_YfmZPma6AwwjA5wuUSVzJjL6Nblcf1/view?usp=sharing); [transcript](https://drive.google.com/file/d/1hweCGRWzlIYqvN1uINPICtZp46KpOY1s/view?usp=sharing); [slides](https://drive.google.com/file/d/15xk-qyd3DFBLBYlTBDegfuZJKElJxuk4/view?usp=sharing) 
1. [Vector embeddings for documents and words](https://drive.google.com/file/d/1L65GHmZxrGanQyc8n6ncLJ91xjcHFVi7/view?usp=sharing); [with captions](https://drive.google.com/file/d/1M1-jH9a6QMBuNqQ5kEgGEW0eseWxV2JS/view?usp=sharing); [transcript](https://drive.google.com/file/d/1Y28Q1_yxTSFdft_MY5UNjbnK2-iC_ZoU/view?usp=sharing); [slides](https://drive.google.com/file/d/1ZOTh6VgchorZxpscuy9ovv-6NVgyyH-B/view?usp=sharing) 
1. [Morphology and weighted finite-state transducers](https://drive.google.com/file/d/1MDj3JUBecLOqCMApOWlxG0ZOxmZcQC20/view?usp=sharing); [with captions](https://drive.google.com/file/d/1zXXPwAFycgIRK-25TctN5IIvo7W2H-ii/view?usp=sharing); [transcript](https://drive.google.com/file/d/16DyBtGwSOUHVcSMN-hvCWsc0awCyX_n2/view?usp=sharing); [slides](https://drive.google.com/file/d/1ejcGyncrh5lSe_P7TRX8Slj_roZUWq2p/view?usp=sharing) 
1. [Sequence labeling and conditional random fields](https://drive.google.com/file/d/1NeLhUxWBBbUSeC5oyz0krxppzlG_OB5V/view?usp=sharing); [with captions](https://drive.google.com/file/d/1uyoeC80ynsVmXjEl2hFZZDWQWHXI8kjF/view?usp=sharing); [transcript](https://drive.google.com/file/d/1G3Ox7tIrjQN9LEV4VX2UL3-lp1VSMANI/view?usp=sharing); [slides](https://drive.google.com/file/d/1eH4OzFMStk1svUZM-8Iiyssb0kOsDrBb/view?usp=sharing) 
1. [Translation and sequence-to-sequence models](https://drive.google.com/file/d/18J0RTgezne5rfu5f9ryaA4Yu1V567q28/view?usp=sharing); [with captions](https://drive.google.com/file/d/1Sej4uNP5bjH0Cot73QKVu5ymHbRWwbN7/view?usp=sharing); [transcript](https://drive.google.com/file/d/1UR1RuQCQHVHn4CL5KabtlnVK7DLnt0WK/view?usp=sharing); [slides](https://drive.google.com/file/d/1BZ6IKDjn12TI8Vg-uf0PvSMZg_C1T9gm/view?usp=sharing) 
1. [Syntax, semantics, and linguistic structure prediction](https://drive.google.com/file/d/1gGXlnv2livCAhH6CK3H-5ij1ZsBNRsOM/view?usp=sharing); [with captions](https://drive.google.com/file/d/1dkGLEjvFupyzBzpb426vkUVC0eMcE6Tu/view?usp=sharing); [transcript](https://drive.google.com/file/d/1ybQeIScWKpOYjq-DC18HWevgn4oDEXwh/view?usp=sharing); [slides](https://drive.google.com/file/d/1KGu3oxTRoLcvKQqPcRhHBuntDCyj6cj4/view?usp=sharing) 


## Grades

You will be evaluated based on individually completed assignments (50%), a project completed in a team of three (40%), and quizzes (10%).  Course staff may grant extra credit (up to 5%) to students who actively and meaningfully engage on the course discussion board.

### Assignments

There will be nine graded assignments, roughly one per week.  You are encouraged to complete all of them on time.  If you submit an assignment within seven days of the due date, the TAs will calculate the grade you would have received if it had been on time, but you will not receive any points.  Your total assignment grade will be calculated as a weighted sum of your nine assignments' grades.  Most assignments will have a weight of one; your best two assignments will be doubly weighted, and your worst two assignments will get a weight of zero.  Because we have built this slack into the grading system, and because you will still receive feedback on work that is slightly late, there will be no exceptions to our policy of zero credit for late work.

### Project 

Details on the projects for CSE 447 and CSE 517 will be added here.  Project deliverables must be turned in on time; there will be zero credit for late submissions.

### Quizzes

There will be quizzes posted on Canvas, roughly once per week.  These are not graded; as long as you submit an attempt, you will earn full credit for the quiz.

## Resources

- Textbook:  [Introduction to Natural Language Processing by Jacob Eisenstein (2019).  MIT Press.](https://www.amazon.com/Introduction-Language-Processing-Adaptive-Computation/dp/0262042843/)


Enrichment lectures and more:

- Claire Cardie, [Information Extraction Through the Years:  How Did We Get Here?](https://slideslive.com/38938634/information-extraction-through-the-years-how-did-we-get-here)
- Yejin Choi, [Intuitive Reasoning as (Un)supervised Neural Generation](https://www.youtube.com/watch?v=h2wzQKRAdA8&ab_channel=MITEmbodiedIntelligence)
- Charles Isbell, [You Can’t Escape Hyperparameters and Latent Variables:  Machine Learning as a Software Engineering Enterprise](https://neurips.cc/virtual/2020/public/invited_16166.html)
- Kathy McKeown, [Rewriting the Past: Assessing the Field through the Lens of Language Generation](https://slideslive.com/38929460/rewriting-the-past-assessing-the-field-through-the-lens-of-language-generation)
- [NLP Highlights](https://soundcloud.com/nlp-highlights) podcast about new research in NLP
- UW NLP [mailing list](https://mailman.cs.washington.edu/mailman/listinfo/uw-nlp) with info about local talks
- [Reading list](https://wammar.github.io/2018sp_uw_cse_599/index.html) from a recent seminar on influential papers in NLP
